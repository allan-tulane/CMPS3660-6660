{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DuPLIZ2XK0Fi",
        "outputId": "842f41fb-a45c-4a67-d786-9e9ec48cf656"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x216 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n",
            "Train Score = 0.5463994060876021\n",
            "Test  Score = 0.5288888888888889\n",
            "Epoch 1:\n",
            "Train Score = 0.6481069042316259\n",
            "Test  Score = 0.6355555555555555\n",
            "Epoch 2:\n",
            "Train Score = 0.7446176688938382\n",
            "Test  Score = 0.76\n",
            "Epoch 3:\n",
            "Train Score = 0.749072011878248\n",
            "Test  Score = 0.7244444444444444\n",
            "Epoch 4:\n",
            "Train Score = 0.8262806236080178\n",
            "Test  Score = 0.8066666666666666\n",
            "Epoch 5:\n",
            "Train Score = 0.8596881959910914\n",
            "Test  Score = 0.84\n",
            "Epoch 6:\n",
            "Train Score = 0.8893838158871566\n",
            "Test  Score = 0.8822222222222222\n",
            "Epoch 7:\n",
            "Train Score = 0.9049740163325909\n",
            "Test  Score = 0.9111111111111111\n",
            "Epoch 8:\n",
            "Train Score = 0.9287305122494433\n",
            "Test  Score = 0.9066666666666666\n",
            "Epoch 9:\n",
            "Train Score = 0.9317000742390498\n",
            "Test  Score = 0.9244444444444444\n",
            "Epoch 10:\n",
            "Train Score = 0.9398663697104677\n",
            "Test  Score = 0.9222222222222223\n",
            "Epoch 11:\n",
            "Train Score = 0.9443207126948775\n",
            "Test  Score = 0.9244444444444444\n",
            "Epoch 12:\n",
            "Train Score = 0.9450631031922792\n",
            "Test  Score = 0.9133333333333333\n",
            "Epoch 13:\n",
            "Train Score = 0.9480326651818857\n",
            "Test  Score = 0.9244444444444444\n",
            "Epoch 14:\n",
            "Train Score = 0.9658500371195249\n",
            "Test  Score = 0.9311111111111111\n",
            "Epoch 15:\n",
            "Train Score = 0.96362286562732\n",
            "Test  Score = 0.9244444444444444\n",
            "Epoch 16:\n",
            "Train Score = 0.9561989606533037\n",
            "Test  Score = 0.9222222222222223\n",
            "Epoch 17:\n",
            "Train Score = 0.9621380846325167\n",
            "Test  Score = 0.9422222222222222\n",
            "Epoch 18:\n",
            "Train Score = 0.9680772086117297\n",
            "Test  Score = 0.9311111111111111\n",
            "Epoch 19:\n",
            "Train Score = 0.9547141796585004\n",
            "Test  Score = 0.9266666666666666\n",
            "Epoch 20:\n",
            "Train Score = 0.9651076466221232\n",
            "Test  Score = 0.9422222222222222\n",
            "Epoch 21:\n",
            "Train Score = 0.9576837416481069\n",
            "Test  Score = 0.9311111111111111\n",
            "Epoch 22:\n",
            "Train Score = 0.9599109131403119\n",
            "Test  Score = 0.9466666666666667\n",
            "Epoch 23:\n",
            "Train Score = 0.9651076466221232\n",
            "Test  Score = 0.9244444444444444\n",
            "Epoch 24:\n",
            "Train Score = 0.9651076466221232\n",
            "Test  Score = 0.9377777777777778\n",
            "Epoch 25:\n",
            "Train Score = 0.9740163325909429\n",
            "Test  Score = 0.94\n",
            "Epoch 26:\n",
            "Train Score = 0.9755011135857461\n",
            "Test  Score = 0.9466666666666667\n",
            "Epoch 27:\n",
            "Train Score = 0.9792130660727543\n",
            "Test  Score = 0.9511111111111111\n",
            "Epoch 28:\n",
            "Train Score = 0.9769858945805494\n",
            "Test  Score = 0.9488888888888889\n",
            "Epoch 29:\n",
            "Train Score = 0.9784706755753526\n",
            "Test  Score = 0.9444444444444444\n",
            "Epoch 30:\n",
            "Train Score = 0.9747587230883444\n",
            "Test  Score = 0.9466666666666667\n",
            "Epoch 31:\n",
            "Train Score = 0.9643652561247216\n",
            "Test  Score = 0.9333333333333333\n",
            "Epoch 32:\n",
            "Train Score = 0.9710467706013363\n",
            "Test  Score = 0.9288888888888889\n",
            "Epoch 33:\n",
            "Train Score = 0.9836674090571641\n",
            "Test  Score = 0.9488888888888889\n",
            "Epoch 34:\n",
            "Train Score = 0.9836674090571641\n",
            "Test  Score = 0.9644444444444444\n",
            "Epoch 35:\n",
            "Train Score = 0.9784706755753526\n",
            "Test  Score = 0.9577777777777777\n",
            "Epoch 36:\n",
            "Train Score = 0.9725315515961396\n",
            "Test  Score = 0.9488888888888889\n",
            "Epoch 37:\n",
            "Train Score = 0.9569413511507052\n",
            "Test  Score = 0.9244444444444444\n",
            "Epoch 38:\n",
            "Train Score = 0.9762435040831478\n",
            "Test  Score = 0.9511111111111111\n",
            "Epoch 39:\n",
            "Train Score = 0.9576837416481069\n",
            "Test  Score = 0.92\n",
            "Epoch 40:\n",
            "Train Score = 0.9821826280623608\n",
            "Test  Score = 0.9422222222222222\n",
            "Epoch 41:\n",
            "Train Score = 0.9747587230883444\n",
            "Test  Score = 0.9377777777777778\n",
            "Epoch 42:\n",
            "Train Score = 0.9799554565701559\n",
            "Test  Score = 0.9533333333333334\n",
            "Epoch 43:\n",
            "Train Score = 0.9851521900519673\n",
            "Test  Score = 0.9488888888888889\n",
            "Epoch 44:\n",
            "Train Score = 0.9829250185597624\n",
            "Test  Score = 0.9622222222222222\n",
            "Epoch 45:\n",
            "Train Score = 0.9866369710467706\n",
            "Test  Score = 0.9377777777777778\n",
            "Epoch 46:\n",
            "Train Score = 0.9881217520415738\n",
            "Test  Score = 0.9644444444444444\n",
            "Epoch 47:\n",
            "Train Score = 0.9910913140311804\n",
            "Test  Score = 0.9644444444444444\n",
            "Epoch 48:\n",
            "Train Score = 0.9903489235337788\n",
            "Test  Score = 0.9533333333333334\n",
            "Epoch 49:\n",
            "Train Score = 0.9910913140311804\n",
            "Test  Score = 0.9533333333333334\n",
            "Epoch 50:\n",
            "Train Score = 0.9903489235337788\n",
            "Test  Score = 0.9533333333333334\n",
            "Epoch 51:\n",
            "Train Score = 0.9881217520415738\n",
            "Test  Score = 0.96\n",
            "Epoch 52:\n",
            "Train Score = 0.9866369710467706\n",
            "Test  Score = 0.9533333333333334\n",
            "Epoch 53:\n",
            "Train Score = 0.9933184855233853\n",
            "Test  Score = 0.96\n",
            "Epoch 54:\n",
            "Train Score = 0.9881217520415738\n",
            "Test  Score = 0.9511111111111111\n",
            "Epoch 55:\n",
            "Train Score = 0.9896065330363771\n",
            "Test  Score = 0.9622222222222222\n",
            "Epoch 56:\n",
            "Train Score = 0.9881217520415738\n",
            "Test  Score = 0.9533333333333334\n",
            "Epoch 57:\n",
            "Train Score = 0.9925760950259837\n",
            "Test  Score = 0.9533333333333334\n",
            "Epoch 58:\n",
            "Train Score = 0.9925760950259837\n",
            "Test  Score = 0.9577777777777777\n",
            "Epoch 59:\n",
            "Train Score = 0.9910913140311804\n",
            "Test  Score = 0.9644444444444444\n",
            "Epoch 60:\n",
            "Train Score = 0.9933184855233853\n",
            "Test  Score = 0.9666666666666667\n",
            "Epoch 61:\n",
            "Train Score = 0.9888641425389755\n",
            "Test  Score = 0.9622222222222222\n",
            "Epoch 62:\n",
            "Train Score = 0.991833704528582\n",
            "Test  Score = 0.9666666666666667\n",
            "Epoch 63:\n",
            "Train Score = 0.991833704528582\n",
            "Test  Score = 0.9644444444444444\n",
            "Epoch 64:\n",
            "Train Score = 0.994060876020787\n",
            "Test  Score = 0.9644444444444444\n",
            "Epoch 65:\n",
            "Train Score = 0.9933184855233853\n",
            "Test  Score = 0.9644444444444444\n",
            "Epoch 66:\n",
            "Train Score = 0.9925760950259837\n",
            "Test  Score = 0.9622222222222222\n",
            "Epoch 67:\n",
            "Train Score = 0.994060876020787\n",
            "Test  Score = 0.9666666666666667\n",
            "Epoch 68:\n",
            "Train Score = 0.9933184855233853\n",
            "Test  Score = 0.9622222222222222\n",
            "Epoch 69:\n",
            "Train Score = 0.9955456570155902\n",
            "Test  Score = 0.9555555555555556\n",
            "Epoch 70:\n",
            "Train Score = 0.9948032665181886\n",
            "Test  Score = 0.9555555555555556\n",
            "Epoch 71:\n",
            "Train Score = 0.9933184855233853\n",
            "Test  Score = 0.9555555555555556\n",
            "Epoch 72:\n",
            "Train Score = 0.9925760950259837\n",
            "Test  Score = 0.9622222222222222\n",
            "Epoch 73:\n",
            "Train Score = 0.991833704528582\n",
            "Test  Score = 0.9622222222222222\n",
            "Epoch 74:\n",
            "Train Score = 0.9925760950259837\n",
            "Test  Score = 0.9688888888888889\n",
            "Epoch 75:\n",
            "Train Score = 0.9933184855233853\n",
            "Test  Score = 0.9488888888888889\n",
            "Epoch 76:\n",
            "Train Score = 0.9925760950259837\n",
            "Test  Score = 0.9533333333333334\n",
            "Epoch 77:\n",
            "Train Score = 0.994060876020787\n",
            "Test  Score = 0.9577777777777777\n",
            "Epoch 78:\n",
            "Train Score = 0.994060876020787\n",
            "Test  Score = 0.9644444444444444\n",
            "Epoch 79:\n",
            "Train Score = 0.9933184855233853\n",
            "Test  Score = 0.96\n",
            "Epoch 80:\n",
            "Train Score = 0.9925760950259837\n",
            "Test  Score = 0.9622222222222222\n",
            "Epoch 81:\n",
            "Train Score = 0.9925760950259837\n",
            "Test  Score = 0.96\n",
            "Epoch 82:\n",
            "Train Score = 0.9933184855233853\n",
            "Test  Score = 0.96\n",
            "Epoch 83:\n",
            "Train Score = 0.9933184855233853\n",
            "Test  Score = 0.96\n",
            "Epoch 84:\n",
            "Train Score = 0.994060876020787\n",
            "Test  Score = 0.9577777777777777\n",
            "Epoch 85:\n",
            "Train Score = 0.9933184855233853\n",
            "Test  Score = 0.9577777777777777\n",
            "Epoch 86:\n",
            "Train Score = 0.9933184855233853\n",
            "Test  Score = 0.9555555555555556\n",
            "Epoch 87:\n",
            "Train Score = 0.9925760950259837\n",
            "Test  Score = 0.9644444444444444\n",
            "Epoch 88:\n",
            "Train Score = 0.9933184855233853\n",
            "Test  Score = 0.9555555555555556\n",
            "Epoch 89:\n",
            "Train Score = 0.994060876020787\n",
            "Test  Score = 0.9577777777777777\n",
            "Epoch 90:\n",
            "Train Score = 0.994060876020787\n",
            "Test  Score = 0.9644444444444444\n",
            "Epoch 91:\n",
            "Train Score = 0.9910913140311804\n",
            "Test  Score = 0.96\n",
            "Epoch 92:\n",
            "Train Score = 0.9925760950259837\n",
            "Test  Score = 0.9644444444444444\n",
            "Epoch 93:\n",
            "Train Score = 0.9948032665181886\n",
            "Test  Score = 0.96\n",
            "Epoch 94:\n",
            "Train Score = 0.9948032665181886\n",
            "Test  Score = 0.9622222222222222\n",
            "Epoch 95:\n",
            "Train Score = 0.9948032665181886\n",
            "Test  Score = 0.9577777777777777\n",
            "Epoch 96:\n",
            "Train Score = 0.9948032665181886\n",
            "Test  Score = 0.9577777777777777\n",
            "Epoch 97:\n",
            "Train Score = 0.9948032665181886\n",
            "Test  Score = 0.9533333333333334\n",
            "Epoch 98:\n",
            "Train Score = 0.9948032665181886\n",
            "Test  Score = 0.9577777777777777\n",
            "Epoch 99:\n",
            "Train Score = 0.9948032665181886\n",
            "Test  Score = 0.96\n",
            "Final Score: 0.96\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random import uniform\n",
        "from sklearn import datasets\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def relu(z):\n",
        "    \"\"\"Rectified linear unit function\"\"\"\n",
        "    return max(0, z)\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n",
        "def vectorized_result(i):\n",
        "    \"\"\"\n",
        "    Return 10 length unit vector with a 1.0 in the jth\n",
        "    position and zeroes elsewhere.  This is used to convert a digit\n",
        "    from 0 -> 9 into a corresponding desired output from the neural\n",
        "    network.\n",
        "    \"\"\"\n",
        "    out = np.zeros((10, 1))\n",
        "    out[i] = 1\n",
        "    return out\n",
        "\n",
        "class SigmoidActivation:\n",
        "    @staticmethod\n",
        "    def activation(z):\n",
        "        return sigmoid(z)\n",
        "    @staticmethod\n",
        "    def activation_prime(z):\n",
        "        return sigmoid_prime(z)\n",
        "\n",
        "class QuadraticCost:\n",
        "    @staticmethod\n",
        "    def C(a, y):\n",
        "        return 0.5 * np.linalg.norm(a - y)**2\n",
        "    @staticmethod\n",
        "    def derivative(z, a, y):\n",
        "        return (a - y) * sigmoid_prime(z)\n",
        "\n",
        "class CrossEntropyCost:\n",
        "    @staticmethod\n",
        "    def C(a, y):\n",
        "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "    @staticmethod\n",
        "    def derivative(z, a, y):\n",
        "        return a - y\n",
        "\n",
        "\n",
        "class MLPClassifier:\n",
        "\n",
        "    def __init__(self, hidden_layer_sizes,\n",
        "                 mini_batch_size=10,\n",
        "                 activation=SigmoidActivation,\n",
        "                 cost=CrossEntropyCost,\n",
        "                 learning_rate=2,\n",
        "                 max_iter=100,\n",
        "                 tol=1E-4,\n",
        "                 n_iter_no_change=10):\n",
        "        \"\"\"\n",
        "        Constructs attributes for neural network class.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        hidden_layer_sizes : list[int]\n",
        "            Sizes of hidden layers in the network. Does not include input or output layer sizes.\n",
        "        mini_batch_size : int\n",
        "            Size of mini-batch used to train the NN.\n",
        "        activation : class\n",
        "            Activation class to calculate activation and derivative of activation. Must have methods of\n",
        "            activation(z) and activation_prime(z).\n",
        "            SigmoidActivation is provided.\n",
        "        cost : class\n",
        "            Cost function class to calculate cost and derivative of cost. Must have methods of\n",
        "            C(a, y) and derivative(z, a, y).\n",
        "        learning_rate : float\n",
        "            Learning rate used in training the neural network.\n",
        "        max_iter : int\n",
        "            Maximum number of epochs used to train the neural network.\n",
        "        tol : float\n",
        "            Tolerance of score change, used to calculate a minimum improvement of score for a training epoch to be\n",
        "            considered improving.\n",
        "        n_iter_no_change : int\n",
        "            Number of consecutive iterations with no score improvement above tol until training is halted.\n",
        "        \"\"\"\n",
        "\n",
        "        self.sizes = hidden_layer_sizes\n",
        "        self.num_layers = len(self.sizes) + 2\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.activation = activation\n",
        "        self.cost = cost\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.n_iter_no_change=n_iter_no_change\n",
        "\n",
        "    def fit(self, X_train, y_train, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Trains the neural network with stochastic gradient descent.\n",
        "\n",
        "        1. Flattens inputs into vectors.\n",
        "        2. Initializes the weights and biases.\n",
        "        3. Performs stochastic gradient descent\n",
        "        \"\"\"\n",
        "        self.x_train = [x.flatten()[..., np.newaxis] for x in X_train]\n",
        "        self.y_train = y_train\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        self.x_test = [x.flatten()[..., np.newaxis] for x in X_test]\n",
        "\n",
        "        # Initialize the weights and biases from standard normal distribution\n",
        "        self.sizes = [self.x_train[0].shape[0]] + self.sizes + [self.y_train[0].shape[0]]\n",
        "        self.biases = [np.random.standard_normal([y, 1]) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.standard_normal([y, x])\n",
        "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "        # Perform SGD\n",
        "        self.stochastic_gradient_descent()\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        \"\"\"Returns the NN output, for input a.\"\"\"\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = self.activation.activation(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    def stochastic_gradient_descent(self):\n",
        "        \"\"\"\n",
        "        Train the NN with mini-batch stochastic gradient descent\n",
        "        \"\"\"\n",
        "\n",
        "        n = len(self.x_train)\n",
        "        epoch = 0\n",
        "        no_change_iter = 0\n",
        "        scores = [0]\n",
        "        score_change = 1\n",
        "        while no_change_iter < self.n_iter_no_change and epoch < self.max_iter:\n",
        "            # Zip x and y training sets into list of tuples for shuffling\n",
        "            training_data = list(zip(self.x_train, self.y_train))\n",
        "            random.shuffle(training_data)\n",
        "            # Split into minibatches\n",
        "            mini_batches = [training_data[k:k+self.mini_batch_size]\n",
        "                            for k in range(0, n, self.mini_batch_size)]\n",
        "            # Iterate through mini batches and train NN\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch)\n",
        "            # Evaluate\n",
        "            train_score = self.evaluate(self.x_train, self.y_train)\n",
        "            score = self.score()\n",
        "\n",
        "            # Report results\n",
        "            print(f\"Epoch {epoch}:\")\n",
        "            print(f\"Train Score = {train_score}\")\n",
        "            print(f\"Test  Score = {score}\")\n",
        "\n",
        "            score_change = score - scores[-1]\n",
        "            scores.append(score)\n",
        "            epoch += 1\n",
        "            # If the NN did not improve, begin counting the number of iterations with no change\n",
        "            if score_change < self.tol:\n",
        "                no_change_iter += 1\n",
        "            else:\n",
        "                no_change_iter = 0\n",
        "\n",
        "    def update_mini_batch(self, mini_batch):\n",
        "        \"\"\"\n",
        "        Update the network's weights and biases by applying gradient\n",
        "        descent using backpropagation to a single mini batch.\n",
        "\n",
        "        mini_batch : list of tuples (x, y)\n",
        "        eta : learning rate\n",
        "        n : total size of training data set\n",
        "        \"\"\"\n",
        "        # Initialize list of partial derivatives\n",
        "        del_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        del_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # Iterate through mini batch, get change in bias/weight partial derivative from backprop\n",
        "        for x, y in mini_batch:\n",
        "            delta_del_b, delta_del_w = self.backprop(x, y)\n",
        "            del_b = [db + ddb for db, ddb in zip(del_b, delta_del_b)]\n",
        "            del_w = [dw + ddw for dw, ddw in zip(del_w, delta_del_w)]\n",
        "\n",
        "        eta = self.learning_rate\n",
        "        self.weights = [w - (eta / len(mini_batch)) * dw\n",
        "                        for w, dw in zip(self.weights, del_w)]\n",
        "        self.biases = [b - (eta / len(mini_batch)) * db\n",
        "                       for b, db in zip(self.biases, del_b)]\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"\n",
        "        Performs backpropagation for input x and output y.\n",
        "        Return a tuple (del_b, del_w) representing the\n",
        "        gradient for the cost function C_x.\n",
        "        del_b & del_w are layer-by-layer lists of numpy arrays, similar\n",
        "        to self.biases and self.weights.\n",
        "        \"\"\"\n",
        "        del_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        del_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # Feedforward Section\n",
        "        activation = x\n",
        "        activations = [x]  # List to store the activations\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = self.cost.derivative(z, activation, y) * self.activation.activation_prime(zs[-1])\n",
        "        del_b[-1] = delta\n",
        "        del_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            activation_derivative = self.activation.activation_prime(z)\n",
        "\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * activation_derivative\n",
        "            del_b[-l] = delta\n",
        "            del_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "\n",
        "        return del_b, del_w\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        \"\"\"\n",
        "        Returns the fraction of correct outputs from the NN.\n",
        "        \"\"\"\n",
        "        results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
        "                   for x, y in zip(x, y)]\n",
        "        return sum([int(x == y) for x, y in results]) / len(x)\n",
        "\n",
        "    def score(self):\n",
        "        \"\"\"\n",
        "        Returns the fraction of correct outputs from the NN.\n",
        "        \"\"\"\n",
        "        results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
        "                   for x, y in zip(self.x_test, self.y_test)]\n",
        "        return sum([int(x == y) for x, y in results]) / len(self.x_test)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load MNIST data, split into X,y train & test\n",
        "    digits = datasets.load_digits()\n",
        "    X = digits.images\n",
        "    y = [vectorized_result(i) for i in digits.target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "    # View 4 digits\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
        "    for ax, image, label in zip(axes, digits.images, digits.target):\n",
        "        ax.set_axis_off()\n",
        "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "        ax.set_title(f\"Digit: {label}\")\n",
        "    plt.show()\n",
        "\n",
        "    # Create & train model\n",
        "    nn = MLPClassifier(hidden_layer_sizes=[50], learning_rate=1.2)\n",
        "    nn.fit(X_train, y_train, X_test, y_test)\n",
        "    accuracy = nn.score()\n",
        "    print(f\"Final Score: {accuracy}\")\n",
        "\n",
        "\n",
        "    #'''"
      ]
    }
  ]
}